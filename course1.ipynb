{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Apprentissage par renforcement"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6107e3b4f303eae4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Le **Reinforcement Learning (RL)**, ou **Apprentissage par Renforcement**, est une méthode d'apprentissage automatique dans laquelle un agent apprend à interagir avec un environnement dans le but de maximiser une récompense cumulative.\n",
    "\n",
    "## Composants de base du RL\n",
    "\n",
    "Le RL repose sur plusieurs concepts clés :\n",
    "\n",
    "1. **Environnement** : Le monde dans lequel l'agent interagit. Il peut être simulé ou réel, et définit les règles de fonctionnement, telles que les actions disponibles, les transitions d'état, et la fonction de récompense.\n",
    "   \n",
    "2. **Agent** : L'entité qui prend des décisions et interagit avec l'environnement pour apprendre la meilleure stratégie.\n",
    "\n",
    "3. **État (state)** : La représentation actuelle de la situation dans laquelle se trouve l'agent.\n",
    "\n",
    "4. **Action** : Les décisions ou mouvements que l'agent peut entreprendre à partir d'un état donné. L'ensemble des actions possibles forme l'espace d'action.\n",
    "\n",
    "5. **Récompense (reward)** : Un signal reçu par l'agent suite à l'exécution d'une action. L'objectif de l'agent est de maximiser cette récompense cumulée sur le long terme.\n",
    "\n",
    "6. **Politique (policy)** : La stratégie de l'agent qui détermine quelle action choisir à chaque étape. La politique peut être **déterministe** (une action spécifique pour chaque état) ou **stochastique** (une distribution de probabilité sur les actions possibles).\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc8ed142d2fd5728"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilisation de OpenAI Gym\n",
    "\n",
    "**OpenAI Gym** est une bibliothèque destinée au développement et à l'évaluation d'algorithmes d'apprentissage par renforcement (Reinforcement Learning, RL). Elle fournit un ensemble d'environnements standardisés permettant aux chercheurs et développeurs de tester et de comparer différentes méthodes d'apprentissage par renforcement.\n",
    "\n",
    "### Fonctionnalités principales\n",
    "- **Environnements variés** : OpenAI Gym propose une large gamme d'environnements, allant des jeux vidéo (comme Atari) à des simulations de contrôle classique (comme le pendule inversé ou le contrôle de robots).\n",
    "- **API simple et standardisée** : Les environnements sont dotés d'une API cohérente pour interagir avec eux via des actions, obtenir des récompenses et observer l'état du système.\n",
    "- **Terminaison** : Le processus se répète jusqu'à la fin de l'épisode, lorsque l'environnement renvoie un signal de terminaison (le jeu est gagné ou perdu, ou un certain nombre d'étapes est atteint)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe289791dab40537"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import gym\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation\n",
    "    import matplotlib.image as mpimg\n",
    "    \n",
    "except ImportError:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:20.520024300Z",
     "start_time": "2024-10-07T12:54:20.189929Z"
    }
   },
   "id": "2b81c6c4c71036e8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les valeurs sont :\n",
      "Position horizontale du chariot : -0.03607102483510971 \n",
      "\n",
      "Vitesse du chariot : -0.01212790422141552 \n",
      "\n",
      "Angle du batôn : -0.028986070305109024 \n",
      "\n",
      "Vitesse angulaire du batôn : 0.03175247833132744 \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = 'human')\n",
    "observation = env.reset()  # init de l environnement crée\n",
    "\n",
    "print(f\"Les valeurs sont :\")\n",
    "print(f\"Position horizontale du chariot : {observation[0][0]} \\n\")\n",
    "print(f\"Vitesse du chariot : {observation[0][1]} \\n\")\n",
    "print(f\"Angle du batôn : {observation[0][2]} \\n\")\n",
    "print(f\"Vitesse angulaire du batôn : {observation[0][3]} \\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:20.938204900Z",
     "start_time": "2024-10-07T12:54:20.520024300Z"
    }
   },
   "id": "12816c05e95edc43"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:20.958141600Z",
     "start_time": "2024-10-07T12:54:20.933199500Z"
    }
   },
   "id": "7788c989ee398d5c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "Discrete(2)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:21.037140600Z",
     "start_time": "2024-10-07T12:54:20.954142400Z"
    }
   },
   "id": "e7426e74df095225"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03631358  0.18339747 -0.02835102 -0.2699331 ]\n",
      "Les valeurs sont :\n",
      "Position horizontale du chariot : -0.036313582211732864 \n",
      "\n",
      "Vitesse du chariot : 0.18339747190475464 \n",
      "\n",
      "Angle du batôn : -0.02835102006793022 \n",
      "\n",
      "Vitesse angulaire du batôn : -0.2699331045150757 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lucas\\miniconda3\\envs\\gymenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "action = 1\n",
    "observation, reward, done, _, info = env.step(action)\n",
    "\n",
    "print(observation)\n",
    "print(f\"Les valeurs sont :\")\n",
    "print(f\"Position horizontale du chariot : {observation[0]} \\n\")\n",
    "print(f\"Vitesse du chariot : {observation[1]} \\n\")\n",
    "print(f\"Angle du batôn : {observation[2]} \\n\")\n",
    "print(f\"Vitesse angulaire du batôn : {observation[3]} \\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:21.055704200Z",
     "start_time": "2024-10-07T12:54:20.964651400Z"
    }
   },
   "id": "4516ea7b46656ac7"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode = \"human\")\n",
    "obs , info = env.reset(seed = int(time.time()))\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    if done or truncated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:23.284183400Z",
     "start_time": "2024-10-07T12:54:20.975163900Z"
    }
   },
   "id": "7b8efb1371702439"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Politique déterministes\n",
    "\n",
    " Pour chaque état, une action unique est toujours choisie.\n",
    "  - **Exemple** : La politique gloutonne est une politique déterministe."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94ee4c25db03c732"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.15 8.754855795500005 24.0 62.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "def policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle<0 else 1\n",
    "\n",
    "tots = []\n",
    "for epochs in range(100):\n",
    "    total_reward = 0\n",
    "    obs, _ = env.reset()\n",
    "    for step in range(100):\n",
    "        action = policy(obs)\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    tots.append(total_reward)\n",
    "\n",
    "\n",
    "print(np.mean(tots), np.std(tots), np.min(tots), np.max(tots))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:23.337353100Z",
     "start_time": "2024-10-07T12:54:23.294201900Z"
    }
   },
   "id": "d8b0e6d113544517"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Politique par réseau de neuronnes\n",
    "\n",
    "Les **politiques par réseaux de neurones** sont des modèles qui permettent de prendre des décisions en environnement complexe et dynamique. Contrairement aux méthodes basées sur des tables ou des formules simples, les réseaux de neurones permettent de traiter un grand nombre d’états et d’actions possibles, ce qui est essentiel dans des environnements à haute dimension ou continus.\n",
    "\n",
    "Les réseaux de neurones permettent de **généraliser** les décisions et d'apprendre des politiques non linéaires, ce qui leur permet de s’adapter aux changements et à l’incertitude dans l’environnement.\n",
    "\n",
    "### 1. **Modèle de politique**\n",
    "Un réseau de neurones est utilisé pour approximer la **politique**, c'est-à-dire la fonction qui prend l'état actuel de l'environnement comme entrée et produit soit une action, soit une distribution de probabilités sur les actions possibles.\n",
    "\n",
    "- **Entrée** : L'état actuel de l'agent dans l'environnement.\n",
    "- **Sortie** : Une action ou une probabilité d'action, que l'agent doit exécuter dans cet état.\n",
    "\n",
    "### 2. **Méthodes basées sur les gradients de politique**\n",
    "Les méthodes **policy gradient** (gradients de politique) optimisent directement la politique en ajustant les paramètres du réseau de neurones pour **maximiser la récompense totale attendue**. L'algorithme ajuste les poids du réseau pour rendre plus probables les actions qui mènent à des récompenses élevées et moins probables celles qui mènent à des faibles récompenses.\n",
    "\n",
    "### Principe\n",
    "- Les actions sont choisies aléatoirement au début, mais avec l’entraînement, les **actions efficaces** deviennent plus probables.\n",
    "- L’apprentissage se fait en calculant la **récompense cumulative** de chaque épisode et en ajustant la politique en conséquence pour maximiser cette récompense.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "848769a73244a9e9"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "n_inputs = 4  # les entrées de la fonction step \n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(6, activation='elu', input_shape=[n_inputs]),\n",
    "    keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "def pg_policy(obs):\n",
    "    left_proba = model.predict(obs[np.newaxis], verbose=0)[0][0]\n",
    "    return int(np.random.rand() > left_proba)\n",
    "\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:27.387880300Z",
     "start_time": "2024-10-07T12:54:23.334350700Z"
    }
   },
   "id": "7c04dae6c4eb0016"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, truncated, info = env.step(int(action))\n",
    "    return obs, reward, done, truncated, grads"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:27.403887900Z",
     "start_time": "2024-10-07T12:54:27.389890Z"
    }
   },
   "id": "85ddf01c3469912"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, truncated, grads = play_one_step(\n",
    "                env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "\n",
    "    return all_rewards, all_grads"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:27.434887800Z",
     "start_time": "2024-10-07T12:54:27.406887Z"
    }
   },
   "id": "660fdcc998383415"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:27.436886400Z",
     "start_time": "2024-10-07T12:54:27.421886300Z"
    }
   },
   "id": "a8275b64ec93ab0c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([-0.28435071, -0.86597718, -1.18910299]),\n array([1.26665318, 1.0727777 ])]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_factor=0.8)\n",
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n",
    "                               discount_factor=0.8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:27.477897Z",
     "start_time": "2024-10-07T12:54:27.436886400Z"
    }
   },
   "id": "42317344e9f2fff"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-07T12:54:27.479895500Z",
     "start_time": "2024-10-07T12:54:27.452890200Z"
    }
   },
   "id": "43ec300c4901a0d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    \n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,discount_factor)\n",
    "    all_mean_grads = []\n",
    "\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        weighted_gradients = []\n",
    "        for episode_index, final_rewards in enumerate(all_final_rewards):\n",
    "            for step, final_reward in enumerate(final_rewards):\n",
    "                variable_gradient = all_grads[episode_index][step][var_index]\n",
    "                weighted_gradient = final_reward * variable_gradient\n",
    "                weighted_gradients.append(weighted_gradient)\n",
    "\n",
    "        mean_grads = tf.reduce_mean(weighted_gradients, axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "        \n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5ff4f6b37921130a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processus de décisions Markoviens\n",
    "\n",
    "Les **chaînes de Markov** sont des processus stochastiques sans mémoire qui évoluent d'un état à un autre de manière aléatoire, selon une probabilité de transition qui dépend uniquement de l'état actuel. Cependant, dans un **processus de décision markovien (MDP)**, un agent peut choisir parmi plusieurs actions, et les probabilités de transition entre les états dépendent des actions choisies. De plus, certaines transitions génèrent des **récompenses** positives ou négatives. L'objectif de l'agent est de maximiser les récompenses cumulées au fil du temps en suivant une **politique optimale**.\n",
    "\n",
    "### Équation d’optimalité de Bellman\n",
    "Bellman a formulé une équation permettant de calculer la **valeur d’état optimale** $V^*(s)$, qui représente la somme des récompenses futures attendues en suivant une politique optimale à partir de l’état $s$.\n",
    "\n",
    "$$ V^*(s) = \\max_{a} \\sum_{s'} T(s, a, s') \\left[ R(s, a, s') + \\gamma V^*(s') \\right] $$\n",
    "\n",
    " **Définition des termes :**\n",
    "- $T(s, a, s')$ : probabilité de transition de l’état $s$ à l’état $s'$ après l’action $a$.\n",
    "- $R(s, a, s')$ : récompense obtenue pour la transition de $s$ à $s'$ avec l’action $a$.\n",
    "- $\\gamma$ : facteur de **rabais** qui pondère l’importance des récompenses futures.\n",
    "\n",
    "### Algorithme d'itération sur la valeur\n",
    "L’algorithme d’**itération sur la valeur** permet d’estimer progressivement les valeurs d’état optimales en mettant à jour les estimations à chaque itération, jusqu’à convergence.\n",
    "\n",
    "$$ V_{k+1}(s) = \\max_{a} \\sum_{s'} T(s, a, s') \\left[ R(s, a, s') + \\gamma V_k(s') \\right] $$\n",
    "\n",
    "### Valeurs Q et itération sur la valeur Q\n",
    "L’**algorithme d’itération sur la valeur Q** permet de calculer la **valeur Q optimale** pour chaque couple état-action $(s, a)$. La valeur $Q^*(s, a)$ représente la somme des récompenses futures pondérées après avoir effectué l’action $a$ à partir de l’état $s$, en supposant que l’agent agit de manière optimale par la suite.\n",
    "\n",
    "$$ Q^*(s, a) = \\sum_{s'} T(s, a, s') \\left[ R(s, a, s') + \\gamma \\max_{a'} Q^*(s', a') \\right] $$\n",
    "\n",
    "### Politique optimale\n",
    "Une fois que les valeurs $Q^*(s, a)$ sont connues, la **politique optimale** $\\pi^*(s)$ est définie comme l'action qui maximise $Q^*(s, a)$ pour chaque état $s$ :\n",
    "\n",
    "$$ \\pi^*(s) = \\arg \\max_a Q^*(s, a) $$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c3c51ee8ff797c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Cet exemple ne s'appuie pas sur l'image ci-dessus\n",
    "\n",
    "proba_transition = [  # shape=[s, a, s']\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],  #s0\n",
    "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]], #s1\n",
    "    [None, [0.8, 0.1, 0.1], None]\n",
    "]\n",
    "\n",
    "recompenses = [  # shape=[s, a, s']\n",
    "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
    "]\n",
    "\n",
    "action_possibles = [[0,1,2], [0,2], [1]]\n",
    "\n",
    "Q_values = np.full((3,3), -np.inf)  # on met -inf pour les actions impossible\n",
    "for num, actions in enumerate(action_possibles):\n",
    "    Q_values[num][actions] = 0.0\n",
    "\n",
    "print(Q_values)\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "for iter in range(50):\n",
    "    Q_preced = Q_values.copy()\n",
    "    for state in range(3):\n",
    "        for action in action_possibles[state]:\n",
    "            Q_values[state, action] = np.sum([proba_transition[state][action][state_prime]*(recompenses[state][action][state_prime]+gamma*np.max(Q_preced[state_prime]))\n",
    "                                             for state_prime in range(3)])\n",
    "print(Q_values)    \n",
    "print(Q_values.argmax(axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "16f862d4f65f0efd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = mpimg.imread('img.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "611032010ffb0de9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "On peut alors déduire les meilleures actions à effectuer en fonction des états. Par exemple, pour l'état s2 entreprendre l'action a1 il gagne environ 50"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "561bfda56c060c45"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apprentissage par différence temporelle \n",
    "\n",
    "Les problèmes d’apprentissage par renforcement avec des actions discrètes peuvent souvent être modélisés à l’aide des processus de décision markoviens. Cependant, l’agent n’a initialement aucune idée des probabilités des transitions (il ne connaît pas \\( T(s, a, s') \\)) et ne sait pas quelles seront les récompenses (il ne connaît pas \\( R(s, a, s') \\)). Il doit tester au moins une fois chaque état et chaque transition pour connaître les récompenses, et il doit le faire à plusieurs reprises s’il veut avoir une estimation raisonnable des probabilités des transitions.\n",
    "\n",
    "L’algorithme d’apprentissage par différence temporelle, ou TD (Temporal Difference Learning), est très proche de l’algorithme d’itération sur la valeur, mais il prend en compte le fait que l’agent n’a qu’une connaissance partielle du MDP. En général, on suppose que l’agent connaît initialement uniquement les états et les actions possibles, rien de plus. Il se sert d’une politique d’exploration, par exemple une politique purement aléatoire, pour explorer le MDP. Au fur et à mesure de sa progression, l’algorithme d’apprentissage TD actualise les estimations des valeurs d’état en fonction des transitions et des récompenses observées.\n",
    "\n",
    "### Algorithme d’apprentissage TD\n",
    "\n",
    "$$\n",
    "V_k(s) \\leftarrow V_k(s) + \\alpha \\left[ r + \\gamma V_k(s') - V_k(s) \\right]\n",
    "$$\n",
    "\n",
    "ou, de façon équivalente :\n",
    "\n",
    "$$\n",
    "V_k(s) \\leftarrow V_k(s) + \\alpha \\delta_k(s, r, s')\n",
    "$$\n",
    "\n",
    "avec\n",
    "\n",
    "$$\n",
    "\\delta_k(s, r, s') = r + \\gamma V_k(s') - V_k(s)\n",
    "$$\n",
    "\n",
    "### Notations\n",
    "\n",
    "- \\( \\alpha \\) : le taux d’apprentissage (par exemple, 0,01).\n",
    "- La cible TD est donnée par :\n",
    "  $$ r + \\gamma V_k(s') $$\n",
    "- L'erreur TD est notée :\n",
    "  $$ \\delta_k(s, r, s') $$\n",
    "\n",
    "On peut y voir une analogie avec la méthode de descente du gradient"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7197c0c543ff7f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apprentissage Q\n",
    "\n",
    "L'algorithme d'apprentissage Q, ou Q-Learning, est une approche qui adapte l'algorithme d'itération sur la valeur Q pour les situations où les probabilités de transition et les récompenses ne sont pas connues au départ. Cet algorithme permet à un agent d'apprendre en jouant, souvent de manière aléatoire, et d'améliorer progressivement ses estimations des valeurs Q. Une fois que l'agent a des estimations de valeur Q suffisamment précises, il peut adopter une politique optimale en choisissant l'action avec la valeur Q la plus élevée, c'est-à-dire en suivant une politique gloutonne.\n",
    "\n",
    "### Algorithme d'apprentissage Q\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right)\n",
    "$$\n",
    "\n",
    "Pour chaque paire état-action \\((s, a)\\), cet algorithme calcule une moyenne mobile des récompenses \\(r\\) que l'agent reçoit lorsqu'il quitte l'état \\(s\\) après avoir effectué l'action \\(a\\), ainsi que la somme des récompenses futures anticipées. Étant donné que la politique cible fonctionnera de manière optimale, cette somme est estimée en prenant le maximum des valeurs Q pour les actions possibles dans l'état suivant \\(s'\\)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa923c083aa88175"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_step(state, action):\n",
    "    probas = proba_transition[state][action] # on recupere le vecteur des probas\n",
    "    next_state = np.random.choice([0,1,2],p=probas)\n",
    "    reward = recompenses[state][action][next_state]\n",
    "    return next_state,reward\n",
    "\n",
    "make_step(2,1)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5b2b622f63ff66ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def policy_exp(state):\n",
    "    return np.random.choice(action_possibles[state])\n",
    "\n",
    "policy_exp(1)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6f51a7041cfb72a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "alpha_0 = 0.05\n",
    "alpha_dim_factor = 0.005\n",
    "gamma = 0.90\n",
    "state = 0\n",
    "\n",
    "for iter in range(500):\n",
    "    action = policy_exp(state) # cas initial\n",
    "    next_state, reward = make_step(state, action)\n",
    "    next_value = np.max(Q_values[next_state])\n",
    "    alpha = alpha_0*(1/1+iter*alpha_dim_factor)\n",
    "    Q_values[state,action] *= (1 - alpha)\n",
    "    Q_values[state,action] += alpha*(reward+gamma*next_value)\n",
    "    state = next_state\n",
    "\n",
    "num_states = 3\n",
    "num_actions = 3\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(Q_values, annot=True, cmap='Blues', cbar=True, \n",
    "            xticklabels=[f'a{i}' for i in range(num_actions)], \n",
    "            yticklabels=[f's{i}' for i in range(num_states)])\n",
    "\n",
    "plt.title('Valeurs Q pour chaque paire État-Action', fontsize=16)\n",
    "plt.xlabel('Actions', fontsize=14)\n",
    "plt.ylabel('États', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9ee7647d04ca76f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apprentissage Q et Convergence\n",
    "\n",
    "Cet algorithme converge vers les valeurs Q optimales, mais il nécessite un nombre considérable d'itérations et potentiellement de nombreux ajustements des hyperparamètres. L'algorithme d'itération sur la valeur Q atteint rapidement la convergence en moins de 20 itérations, tandis que l'algorithme d'apprentissage Q (illustré à droite) requiert environ 8 000 itérations pour converger. Il est évident que l'absence de connaissances sur les probabilités de transition ou les récompenses rend la recherche de la politique optimale beaucoup plus complexe !\n",
    "\n",
    "L'algorithme d'apprentissage Q est considéré comme un algorithme de politique hors ligne (off-policy), car la politique qui est entraînée n'est pas nécessairement celle qui est mise en œuvre. Dans le code précédent, la politique appliquée (la politique d'exploration) est entièrement aléatoire, tandis que la politique entraînée privilégie toujours les actions avec les valeurs Q les plus élevées. À l'inverse, l'algorithme des gradients de politique est un algorithme de politique en ligne (on-policy), car il explore l'environnement en utilisant la politique actuellement entraînée. Il est plutôt surprenant que l'apprentissage Q puisse découvrir la politique optimale simplement en observant un agent agir de manière aléatoire, comme si l'on pouvait apprendre à jouer au golf en regardant un singe ivre jouer.\n",
    "\n",
    "### Politiques d'exploration\n",
    "\n",
    "L'apprentissage Q n'est efficace que si la politique d'exploration couvre suffisamment l'espace d'état du MDP. Bien qu'une politique aléatoire finisse par visiter chaque état et chaque transition à plusieurs reprises, elle pourrait prendre un temps excessif pour y parvenir. Une stratégie plus efficace consiste à utiliser la politique ε-greedy (où ε représente epsilon) : à chaque étape, elle agit de manière aléatoire avec une probabilité ε, ou choisit la meilleure action avec une probabilité de 1−ε (c'est-à-dire en sélectionnant l'action avec la valeur Q la plus élevée). Par rapport à une politique totalement aléatoire, la politique ε-greedy a l'avantage d'explorer davantage les zones prometteuses de l'environnement, tout en continuant à visiter des régions moins connues à mesure que les estimations des valeurs Q s'améliorent. Il est courant de commencer avec une valeur élevée pour ε (par exemple, 1,0) puis de la réduire progressivement (jusqu'à environ 0,05).\n",
    "\n",
    "Au lieu d'une exploration totalement aléatoire, une autre approche consiste à inciter la politique d'exploration à essayer des actions qu'elle a moins souvent testées auparavant. Cela peut être réalisé en ajoutant un bonus aux estimations de la valeur Q (voir l'équation 10.6).\n",
    "\n",
    "### Apprentissage Q avec une fonction d'exploration\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow (1 - \\alpha) Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s',a') + f(Q,N) \\right)\n",
    "$$\n",
    "\n",
    "Dans cette équation :\n",
    "- $N(s', a')$ représente le nombre de fois où l'action $a'$ a été choisie dans l'état $s'$.\n",
    "- $f(Q, N)$ est une fonction d'exploration, par exemple $f(Q, N) = Q + \\frac{\\kappa}{1 + N}$, où $\\kappa$ est un hyperparamètre de curiosité qui mesure l'attrait de l'agent pour l'inconnu."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e269c71b01f7b72"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Réseaux Q profond\n",
    "\n",
    "Les réseaux Q profonds (Deep Q-Networks, DQN) sont une extension des algorithmes d'apprentissage Q qui intègrent des réseaux de neurones profonds pour approximer les valeurs Q. Cette approche a été introduite par les chercheurs de DeepMind dans le cadre de leur travail sur le jeu Atari, où elle a montré des performances remarquables.\n",
    "\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Une des principales forces des DQN est leur capacité à traiter des situations où il existe un nombre énorme de possibilités. Par exemple, dans des jeux vidéo ou des environnements de simulation complexes, le nombre d'états possibles peut être astronomique. Les DQN permettent de généraliser l'apprentissage sur ces espaces vastes en apprenant à partir d'exemples et en adaptant les poids du réseau de neurones au fil du temps."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9656f91bc420010"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apprentissage Q par Approximation\n",
    "\n",
    "La clé réside dans la définition d'une fonction \\( Q_q(s, a) \\) qui permet d'approcher la valeur Q pour tout couple état-action \\((s, a)\\), tout en utilisant un nombre de paramètres raisonnable, spécifié par le vecteur de paramètres \\( q \\). Cette méthode est désignée sous le terme **apprentissage Q par approximation**.\n",
    "\n",
    "Historiquement, on conseillait d’utiliser des combinaisons linéaires de caractéristiques définies manuellement à partir de l’état (comme la distance aux fantômes les plus proches, leur orientation, etc.) pour estimer les valeurs Q. Toutefois, en 2013, DeepMind a démontré que l'emploi de réseaux de neurones profonds peut offrir des performances nettement supérieures, surtout dans le cadre de problèmes complexes. Cela permet également d'économiser le temps nécessaire pour développer des caractéristiques efficaces.\n",
    "\n",
    "Un réseau de neurones utilisé pour estimer les valeurs Q est connu sous le nom de **réseau Q profond** (DQN, Deep Q-Network), et l'application de ce DQN dans le cadre de l'apprentissage Q par approximation est appelée **apprentissage Q profond** (Deep Q-Learning).\n",
    "\n",
    "### Entraînement d'un DQN\n",
    "\n",
    "Comment peut-on s'y prendre pour entraîner un DQN ? Pour le comprendre, examinons comment le DQN estime la valeur Q pour un couple état-action \\((s, a)\\). Selon l'équation de Bellman, il est essentiel que cette estimation se rapproche le plus possible de la récompense \\( r \\) que l’on reçoit après avoir effectué l'action \\( a \\) dans l'état \\( s \\), plus la somme des récompenses futures actualisées que l'on peut anticiper en adoptant une stratégie optimale par la suite.\n",
    "\n",
    "Pour obtenir cette estimation, il suffit d'appliquer le DQN à l'état suivant \\( s' \\) et d'évaluer toutes les actions possibles \\( a' \\). Cela nous permet de calculer la valeur Q pour chaque action suivante. En sélectionnant la valeur maximale (en supposant que l'on agisse de manière optimale) et en y appliquant un facteur de rabais, nous pouvons obtenir une estimation de la somme des récompenses futures actualisées. En y ajoutant la récompense \\( r \\), nous définissons ainsi une **valeur Q cible** \\( Q(s, a) \\) pour le couple état-action \\((s, a)\\) (voir l’équation 10.7).\n",
    "\n",
    "#### Valeur Q cible\n",
    "$$\n",
    "Q(s, a) = r + \\gamma \\max_{a'} Q(s', a')\n",
    "$$\n",
    "\n",
    "Une fois que nous avons cette valeur Q cible, nous sommes en mesure d'effectuer une étape d’entraînement en recourant à n'importe quel algorithme de descente de gradient. En pratique, notre objectif est généralement de réduire l'erreur quadratique entre la valeur Q estimée \\( Q(s, a) \\) et la valeur Q cible. Pour cela, nous pouvons également utiliser la perte de Huber afin de diminuer la sensibilité de l'algorithme aux erreurs considérables.\n",
    "\n",
    "Nous avons ainsi couvert les bases de l'apprentissage Q profond. Passons maintenant à la mise en œuvre de cette approche pour résoudre le problème de l'environnement CartPole.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d96fcda785adb81f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "#print(env.observation_space.shape) # renvoie 4\n",
    "input_shape = [4] \n",
    "#print(env.action_space) # Discrete(2)\n",
    "output_shape = 2\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation='relu', input_shape = input_shape),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(output_shape)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "55bf085a8c74e193"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, epsilon = 0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(2)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        print(Q_values.shape)\n",
    "        return np.argmax(Q_values[0])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a00e1b561d78db05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen = 200)\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones, truncateds = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(6)\n",
    "    ]\n",
    "    return states, actions, rewards, next_states, dones, truncateds"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "838dd5fe2ba3f7b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy(state,epsilon)\n",
    "    next_state,reward,done,truncated,info = env.step(action)\n",
    "    replay_buffer.append((state,action,reward,next_state,done,truncated))\n",
    "    return next_state,reward,done,truncated,done"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1b5fb9f1ee4eb798"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_one_step(batch_size):\n",
    "    exp = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = exp\n",
    "    next_q_values = model.predict(next_states, verbose=0)\n",
    "    max_q = np.max(next_q_values, axis=1)\n",
    "    target_q = (rewards + (1-dones)*discount_factor*max_q)\n",
    "    mask = tf.one_hot(actions,output_shape)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        all_q = model.predict(states)\n",
    "        Q_values = tf.reduce_sum(all_q*mask, axis=1,keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_q,Q_values))\n",
    "    gradients = tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "841263b7815c1d4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rewards = [] \n",
    "best_score = 0\n",
    "\n",
    "for episode in range(600):\n",
    "    obs, info = env.reset()    \n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
    "        if done or truncated:\n",
    "            break\n",
    "    rewards.append(step)\n",
    "    if step >= best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = step\n",
    "\n",
    "    if episode > 50:\n",
    "        training_one_step(batch_size)\n",
    "\n",
    "model.set_weights(best_weights) "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3fcfb13a03d558a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Somme des recompenses\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"dqn_rewards_plot\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "83596ea4845f1caa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variantes de l'apprentissage Q profond\n",
    "\n",
    "### Cibles de la valeur Q Fixées\n",
    "\n",
    "Dans l'algorithme de base de l'apprentissage Q profond, le même modèle est utilisé pour faire des prédictions et pour définir ses propres cibles. Cela peut entraîner une rétroaction instable, provoquant des problèmes tels que des divergences ou des oscillations. Pour résoudre ce problème, DeepMind a proposé, dans son article de 2013, d'utiliser deux DQN au lieu d'un.\n",
    "\n",
    "- **Modèle en ligne** : Ce modèle apprend à chaque étape et détermine les actions de l'agent.\n",
    "- **Modèle cible** : Utilisé uniquement pour définir les cibles, il est un clone du modèle en ligne. On peut créer ce modèle cible en utilisant la méthode suivante :\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44fd9c12a0ecb85c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "target = tf.keras.models.clone_model(model) \n",
    "target.set_weights(model.get_weights())\n",
    "\n",
    "rewards = [] \n",
    "best_score = 0\n",
    "\n",
    "def training_one_step(batch_size):\n",
    "    exp = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = exp\n",
    "    next_q_values = target.predict(next_states, verbose=0)\n",
    "    max_q = np.max(next_q_values, axis=1)\n",
    "    target_q = (rewards + (1-dones)*discount_factor*max_q)\n",
    "    mask = tf.one_hot(actions,output_shape)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        all_q = model.predict(states)\n",
    "        Q_values = tf.reduce_sum(all_q*mask, axis=1,keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_q,Q_values))\n",
    "    gradients = tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    \n",
    "for episode in range(600):\n",
    "    obs, info = env.reset()    \n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
    "        if done or truncated:\n",
    "            break\n",
    "    rewards.append(step)\n",
    "    if step >= best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = step\n",
    "\n",
    "    if episode % 50:\n",
    "        target.set_weights(model.get_weights())\n",
    "\n",
    "model.set_weights(best_weights) "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "77e9aba04f5b7af2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Somme des recompenses\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"dqn_rewards_plot_2DNN\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b9cb3541fdb72615"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Double Deep-Q-Network\n",
    "\n",
    "En 2015, des chercheurs de DeepMind ont amélioré leur algorithme DQN, ce qui a permis d'optimiser ses performances et de stabiliser le processus d'entraînement. Cette version améliorée est désignée sous le nom de **DQN double**.\n",
    "\n",
    "### Problème de Surestimation\n",
    "\n",
    "Cette amélioration repose sur une observation clé : le réseau cible a une tendance à surestimer les valeurs Q. En considérant que toutes les actions sont égales, on devrait s'attendre à ce que les valeurs Q estimées par le modèle cible soient cohérentes. Toutefois, étant donné qu'il s'agit d'approximations, certaines valeurs peuvent apparaître légèrement plus élevées que d'autres, souvent par accident.\n",
    "\n",
    "Le modèle cible opte systématiquement pour la valeur Q la plus élevée, ce qui peut entraîner une surestimation par rapport à la valeur Q moyenne, faussant ainsi l'estimation réelle.\n",
    "\n",
    "### Solution Proposée\n",
    "\n",
    "Pour remédier à ce biais de surestimation, les chercheurs ont suggéré d'employer le modèle en ligne pour sélectionner les meilleures actions à venir, tandis que le modèle cible serait uniquement utilisé pour évaluer les valeurs Q associées à ces actions optimales."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0858aa323d0e41"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
